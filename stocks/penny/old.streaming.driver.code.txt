package com.fractal.spark.pfsense_etl
import java.util

import com.fractal.mdtsdb.client.api.MdtsdbClient
import com.fractal.pfsense.core.{LogRouter, MDTSDBUtils, ParamUtils}
import com.fractal.pfsense.core.ParamUtils.ParamsObj
import com.typesafe.config.Config
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Duration, Seconds, StreamingContext}

import scala.util.Try
import scala.util.Success





/**
  * Created by mdb on 10/16/17.
  * https://spark.apache.org/docs/2.1.0/streaming-kafka-0-10-integration.html
  */
object PFSenseStreamingDriver {
  var topic:String = null
  var group:String = null
  var broker:String  = null
  var numThreads:Int = 0
  var mdtsdbUrl:String = null
  var mdtsdbPort:Int = 0
  var clAdmKey:String  = null
  var clAdmSecretKey:String  = null
  var swlName:String  = null
  var swlAppKey:String  = null
  var swlSecretKey:String  = null
  var paramsObj: ParamsObj = null
  var _config:Config = null

  def main(args:Array[String]): Unit ={
    val sparkMaster = "local[*]"
    val sparkAppName = "PFSense  Service Streaming"
    val sparkConf = new SparkConf()
    sparkConf.setAppName(sparkAppName)
    sparkConf.setMaster(sparkMaster)
    sparkConf.set("spark.streaming.backpressure.enabled","true")
    val ssc = new StreamingContext(sparkConf,Seconds(5))
    val json = args.mkString("")
    ParamsProcessor.setParamsFromJson(json)
    _config = ParamsProcessor.getConfigObj()
    useStreamingContext(ssc, _config)

  }


  def useStreamingContext(ssc: StreamingContext, _config:Config) = {




  /*
  def useStreamingContext(ssc: StreamingContext, json: String) = {
    if(json.length > 0){
      ParamUtils.setParams(json)
      paramsObj = ParamUtils.getParams()
      topic = paramsObj.topic.trim()
      group = paramsObj.group.trim()
      broker = paramsObj.broker.trim()
      numThreads = paramsObj.numThreads
      clAdmKey = paramsObj.clAdmKey.trim()
      clAdmSecretKey = paramsObj.clAdmKeySecret.trim()
      mdtsdbUrl = paramsObj.mdtsdbUrl.trim()
      mdtsdbPort = paramsObj.mdtsdbPort
      swlName = paramsObj.swlName.trim()
      swlAppKey = paramsObj.swlKey.trim()
      swlSecretKey = paramsObj.swlSecretKey.trim()

    }else{
      MongoDBUtils.setMongoConfig()
      val keyMap: util.Map[String, String] = MongoDBUtils.keysFromMongo()
      broker =  keyMap.get("broker")
      group = keyMap.get("group")
      topic = keyMap.get("topic")
      numThreads = keyMap.get("numThreads").toInt
      mdtsdbUrl = keyMap.get("mdtsdbUrl")
      mdtsdbPort =  keyMap.get("mdtsdbPort").toInt
      clAdmKey = keyMap.get("clAdmKey")
      clAdmSecretKey =  keyMap.get("clAdmSecretKey")
      swlName = keyMap.get("name")
      swlAppKey = keyMap.get("swlAppKey")
      swlSecretKey = keyMap.get("swlSecretKey")
      paramsObj =  ParamUtils.ParamsObj(topic,
        group,
        broker,
        numThreads,
        mdtsdbUrl,
        mdtsdbPort,
        clAdmKey,
        clAdmSecretKey,
        swlName,
        swlAppKey,
        swlSecretKey)
    }
    */

    //println( "params object inside useStreamingContext : " + paramsObj)

    /*
    WorkerBee.setParams(paramsObj)
    val tsclient = WorkerBee.getParams()
    if(tsclient!=null){
      println("tsclient App Key : " + tsclient.getAppKey())
      println("tsclient Secret Key : " + tsclient.getSecretKey())
    }
    */

    WorkerBee.setParams(paramsObj)
    val tsclient = WorkerBee.getParams()
    if(tsclient!=null){
      println("tsclient App Key : " + tsclient.getAppKey())
      println("tsclient Secret Key : " + tsclient.getSecretKey())
    }


    import _root_.kafka.serializer.StringDecoder
    val kafkaParams: Map[String, String] = Map("metadata.broker.list" -> broker)
    val kafkaTopics: Set[String] = Set(topic)
    println("KafkaParamMap :  " + kafkaParams )
    println("kafkaTopics : " + kafkaTopics)

    val recordsInputDStream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, kafkaTopics)
    recordsInputDStream.persist()
    recordsInputDStream.foreachRDD{
      rdd => rdd.foreach {
        WorkerBee.foo
      }
    }
    recordsInputDStream.saveAsTextFiles("/tmp/pfsense/log")
    ssc.start()
    ssc.awaitTermination()

  }
  
  /**
   * Ensure that the config (and context) for this job is valid.
   * Returns either a Success[String] wrapping a message of completion or
   * a Failure wrapping a (potentially custom) Exception indicating (in its message)
   * at least one area of the config that failed to validate. 
   */
  def validate(ssc: StreamingContext, config: String): Try[String] = {
    // TODO : Michael, perform complete validation
    Success("Blindly accepting pfSense configuration!")
  }

}

object WorkerBee {
  var mtsdbAppClient: MdtsdbClient = null
  def setParams(params:ParamUtils.ParamsObj): Unit ={
    mtsdbAppClient = MDTSDBUtils.createSwimLaneClient(params)
  }
  def getParams(): MdtsdbClient = {
      return mtsdbAppClient
  }
  def foo(line: (String, String)) = {
    val recordMap: Option[util.LinkedHashMap[String, String]] = LogRouter.routeRecord(line._2)
    if (recordMap != None) {
      LogRouter.sensorRouter(mtsdbAppClient,recordMap)
    }
  }
}




